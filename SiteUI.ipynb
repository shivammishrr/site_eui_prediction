{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr5CWiSizTFb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from category_encoders import TargetEncoder\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import Ridge, Lasso, RANSACRegressor\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import optuna\n",
        "\n",
        "import tqdm\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "import shap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpYmWqC3zTFd"
      },
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">⬇️ Import Data</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIBXyn8HzTFf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "713b9008-2d9b-4bd3-db67-bdfaf83486c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2095ecaa8ffb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/content/train_dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"/content/x_test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train_dataset.csv'"
          ]
        }
      ],
      "source": [
        "train = pd.read_csv(r\"/content/train_dataset.csv\")\n",
        "test = pd.read_csv(r\"/content/x_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ib9nAX1SzTFg"
      },
      "outputs": [],
      "source": [
        "train_df = train.copy()\n",
        "test_df = test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLUS7dPJzTFh"
      },
      "outputs": [],
      "source": [
        "print(train_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlpn_uZYzTFi"
      },
      "outputs": [],
      "source": [
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvQLHcNJzTFj"
      },
      "outputs": [],
      "source": [
        "train_df.rename(columns={\n",
        "    'Year_Factor': 'year_factor',\n",
        "    'State_Factor': 'state_factor',\n",
        "    'ELEVATION': 'elevation',\n",
        "    'id': 'building_id'\n",
        "}, inplace=True)\n",
        "\n",
        "test_df.rename(columns={\n",
        "    'Year_Factor': 'year_factor',\n",
        "    'State_Factor': 'state_factor',\n",
        "    'ELEVATION': 'elevation',\n",
        "    'id': 'building_id'\n",
        "}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjA2pMtuzTFk"
      },
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">📊 Exploratory Data Analysis</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1dFALhxzTFl"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n",
        "    <font size='5pt'><b>Understanding features:</b></font>\n",
        "\n",
        "year_Factor: anonymized year in which the weather and energy usage factors were observed\n",
        "\n",
        "state_Factor: anonymized state in which the building is located\n",
        "\n",
        "building_class: building classification\n",
        "\n",
        "facility_type: building usage type\n",
        "\n",
        "floor_area: floor area (in square feet) of the building\n",
        "\n",
        "year_built: year in which the building was constructed\n",
        "\n",
        "energy_star_rating: the energy star rating of the building\n",
        "\n",
        "elevation: elevation of the building location\n",
        "\n",
        "january_min_temp: minimum temperature in January (in Fahrenheit) at the location of the building\n",
        "\n",
        "january_avg_temp: average temperature in January (in Fahrenheit) at the location of the building\n",
        "\n",
        "january_max_temp: maximum temperature in January (in Fahrenheit) at the location of the building\n",
        "\n",
        "{Similarly for all other months}\n",
        "\n",
        "cooling_degree_days: cooling degree day for a given day is the number of degrees where the daily average temperature exceeds 65 degreees Fahrenheit. Each month is summed to produce an annual total at the location of the building.\n",
        "\n",
        "heating_degree_days: heating degree day for a given day is the number of degrees where the daily average temperature falls under 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building.\n",
        "\n",
        "precipitation_inches: annual precipitaion in inches at the location of the building\n",
        "\n",
        "snowfall_inches: annual snowfall in inches at the location of the building\n",
        "\n",
        "snowdepth_inches: annual snowfall in inches at the location of the building\n",
        "\n",
        "avg_temp: average temperature over a year at the location of the building\n",
        "\n",
        "days_below_30F: total number of days below 30 degrees Fahrenheit at the location of the building\n",
        "\n",
        "days_below_20F: total number of days below 20 degrees Fahrenheit at the location of the building  \n",
        "\n",
        "days_below_10F: total number of days below 10 degrees Fahrenheit at the location of the building  \n",
        "\n",
        "days_below_0F: total number of days below 0 degrees Fahrenheit at the location of the building  \n",
        "\n",
        "days_above_80F: total number of days above 80 degrees Fahrenheit at the location of the building  \n",
        "\n",
        "days_above_90F: total number of days above 90 degrees Fahrenheit at the location of the building  \n",
        "\n",
        "days_above_100F: total number of days above 100 degrees Fahrenheit at the location of the building  \n",
        "\n",
        "days_above_110F: total number of days above 110 degrees Fahrenheit at the location of the building\n",
        "\n",
        "direction_max_wind_speed: wind direction for maximum wind speed at the location of the building. Given in 360-degree compass point directions (e.g. 360 = north, 180 = south, etc.).\n",
        "\n",
        "direction_peak_wind_speed: wind direction for peak wind gust speed at the location of the building. Given in 360-degree compass point directions (e.g. 360 = north, 180 = south, etc.).\n",
        "\n",
        "max_wind_speed: maximum wind speed at the location of the building\n",
        "\n",
        "days_with_fog: number of days with fog at the location of the building\n",
        "\n",
        "building_id: building id\n",
        "\n",
        "<br>\n",
        "<font size='5pt'><b>Target</b></font><br>\n",
        "site_eui: Site Energy Usage Intensity is the amount of heat and electricity consumed by a building as reflected in utility bills"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yeFOgL1RzTFm"
      },
      "outputs": [],
      "source": [
        "numerical_feature = [feature for feature in train_df.columns if train_df[feature].dtypes != 'O']\n",
        "discrete_feature = [feature for feature in train_df.columns if len(train_df[feature].unique()) < 25]\n",
        "continuous_feature = [feature for feature in numerical_feature if feature not in  discrete_feature]\n",
        "categorical_feature = [feature for feature in train_df.columns if feature not in numerical_feature]\n",
        "\n",
        "print(\"Numerical Feature Count {}\".format(len(numerical_feature)))\n",
        "print(\"Discrete Feature Count {}\".format(len(discrete_feature)))\n",
        "print(\"Continuous Feature Count {}\".format(len(continuous_feature)))\n",
        "print(\"Categorical Feature Count {}\".format(len(categorical_feature)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiTaD3hFzTFm"
      },
      "outputs": [],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOPFp3TbzTFn"
      },
      "outputs": [],
      "source": [
        "print(train_df.duplicated().sum())\n",
        "print(test_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGP6ZpBzzTFn"
      },
      "outputs": [],
      "source": [
        "print([col for col in train_df if train_df[col].nunique() == 1])\n",
        "print([col for col in test_df if test_df[col].nunique() == 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqPjbyIrzTFo"
      },
      "outputs": [],
      "source": [
        "def missing_values_table(df):\n",
        "    # Total missing values by column\n",
        "    mis_val = df.isnull().sum()\n",
        "\n",
        "    # Percentage of missing values by column\n",
        "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
        "\n",
        "    # build a table with the thw columns\n",
        "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "\n",
        "    # Rename the columns\n",
        "    mis_val_table_ren_columns = mis_val_table.rename(\n",
        "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
        "\n",
        "    # Sort the table by percentage of missing descending\n",
        "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
        "        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
        "    '% of Total Values', ascending=False).round(1)\n",
        "\n",
        "    # Print some summary information\n",
        "    print(\"Your selected dataframe has \" + str(df.shape[1]) + \"columns.\\n\"\n",
        "        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
        "          \" columns that have missing values.\")\n",
        "\n",
        "    # Return the dataframe with missing information\n",
        "    return mis_val_table_ren_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXOiXNSjzTFo"
      },
      "outputs": [],
      "source": [
        "print(\"Train set columns with null values: \")\n",
        "print(list(train_df.columns[train_df.isnull().any()]))\n",
        "print('===========================================')\n",
        "# Missing values for training data\n",
        "missing_values_train = missing_values_table(train_df)\n",
        "missing_values_train[:20].style.background_gradient(cmap='Blues')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFlmGFUNzTFo"
      },
      "outputs": [],
      "source": [
        "print(\"Test set columns with null values: \")\n",
        "print(list(test_df.columns[test_df.isnull().any()]))\n",
        "print('===========================================')\n",
        "# Missing values for test data\n",
        "missing_values_test = missing_values_table(test_df)\n",
        "missing_values_test[:20].style.background_gradient(cmap='Greens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0cEatJRzTFp"
      },
      "outputs": [],
      "source": [
        "test_df[['year_factor', 'days_above_110F']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlQw3iAezTFp"
      },
      "source": [
        "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
        "<h3 align=\"left\"><font color='#DEB887'>💡 Observations:</font></h3>\n",
        "\n",
        "1) 'year_built', 'energy_star_rating', 'direction_max_wind_speed', 'direction_peak_wind_speed', 'max_wind_speed', 'days_with_fog' variables have NA values.\n",
        "2) Data contains 75757 samples and 64 features. We have 3 categorical and rest numerical features. Target is numerical.\n",
        "3) There are no duplicates in individual dataset, however there might be duplicates in the combined version\n",
        "4) there are no constant columns in train dataset, but test dataset has 2 constant columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jgxTobAzTFp"
      },
      "source": [
        "#### Combine the datasets for the visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyKjHo2VzTFp"
      },
      "outputs": [],
      "source": [
        "test['site_eui'] = np.nan # 63\n",
        "test['dataset'] = 'test'\n",
        "train['dataset'] = 'train'\n",
        "\n",
        "# train = commondf[\"dataset\"] == \"train\"\n",
        "\n",
        "df_all = pd.concat([train, test], axis=0, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hbq74zrzTFq"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n",
        "\n",
        "The dataset contains time variable, 'Year_Factor', that has values from 1 to 7. Assuming the values are consecutive years, 'train' dataset contains the first 6 years and 'test' set contains the 7th year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izDt6jd9zTFq"
      },
      "outputs": [],
      "source": [
        "ax = sns.catplot(x='Year_Factor',\n",
        "                 y = 'id',\n",
        "                 data=df_all.groupby(['dataset','Year_Factor']).agg({'id':'count'}).reset_index(),\n",
        "                 hue='dataset',\n",
        "                 kind='bar',\n",
        "                 aspect=2,\n",
        "                 height=4)\\\n",
        ".set(title='The number of data points by year',\n",
        "     ylabel=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nibKsx_uzTFq"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n",
        "'train' set contains data points of 7 states, however, 'test' doesn't have any data point from 'State_6'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhyRU5_3zTFq"
      },
      "outputs": [],
      "source": [
        "ax = sns.relplot(x='State_Factor',\n",
        "                 y = 'id',\n",
        "                 data=df_all.groupby(['dataset','State_Factor']).agg({'id':'count'}).reset_index(),\n",
        "                 hue='dataset',\n",
        "                 aspect=2,\n",
        "                 height=4,\n",
        "                 s=50,\n",
        "                 alpha=0.9\n",
        "                 )\\\n",
        ".set(title = \"The number of data points by States\",\n",
        "     ylabel=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IogwL1nzTFr"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,3, figsize=(20,5), sharey=True)\n",
        "fig.suptitle(\"The number of data points by dataset\")\n",
        "\n",
        "sns.barplot(x='dataset',\n",
        "            y='id',\n",
        "            data=df_all.groupby(['dataset','building_class']).agg({'id':'count'}).reset_index(),\n",
        "            hue='building_class',\n",
        "            ax=ax[0],\n",
        "            ci=False)\n",
        "\n",
        "for e,s in enumerate(df_all['building_class'].unique(),1):\n",
        "    sns.barplot(x='State_Factor',\n",
        "                y='id',\n",
        "                data=df_all[df_all['building_class']==s].groupby(['dataset','State_Factor']).agg({'id':'count'}).reset_index(),\n",
        "                hue='dataset',\n",
        "                ax=ax[e],\n",
        "                ci=False)\n",
        "    ax[e].set_title(s)\n",
        "    ax[e].set_ylabel(\"\")\n",
        "    ax[e].set_xlabel(\"\")\n",
        "\n",
        "ax[0].set_title(\"By building class\")\n",
        "ax[0].set_ylabel(\"\")\n",
        "ax[0].set_xlabel(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ovUyF5yzTFr"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">Both residential and commercial buildings are present in both datasets. `train` data contains more residential buildings compared to `test` set which is due to the number of `State_6` buildings in `train` set which seems to add to the number. Apart from `State_6`, the number of commercial buildings are higher than the residential buildings and `State_10` only contains commercial buildings. The type of building and state could by important factors in determining `EUI`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOnAZ6IFzTFr"
      },
      "outputs": [],
      "source": [
        "ax = sns.relplot(x='facility_type',\n",
        "                 y='id',\n",
        "                 data=df_all.groupby(['dataset','facility_type']).agg({'id':'count'}).reset_index(),\n",
        "                 hue='dataset',\n",
        "                 style='dataset',\n",
        "                 aspect=3,\n",
        "                 height=5,\n",
        "                 s=50,\n",
        "                 alpha=0.9\n",
        "                 )\\\n",
        ".set(title = \"The number of data points by Facility type\",\n",
        "     ylabel=None)\n",
        "\n",
        "ax.set_xticklabels(rotation=90)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfVCC9aVzTFr"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">All facility types are present in both datasets. Almost 90% of residential `Multifamily_uncategorized` buildings and 50% of `Office_uncategorized` are in `State_6` of the training set. Not quite sure how `State_6` will affect the model learning and prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rk2FNa39zTFs"
      },
      "outputs": [],
      "source": [
        "df_all['year_built'].value_counts().index.sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Oo9I2wzTFs"
      },
      "outputs": [],
      "source": [
        "temp = df_all[['year_built']].fillna(2029).replace({0:2029}).astype('category').value_counts().reset_index().rename({0:'count'},axis=1)\\\n",
        "            .sort_values('year_built')\n",
        "# temp['year_built'] = temp['year_built'].astype('category')\n",
        "fig, ax = plt.subplots(figsize=(15,5))\n",
        "\n",
        "ax=plt.bar(temp['year_built'],\n",
        "           temp['count']\n",
        "           )\n",
        "\n",
        "fig.suptitle(f\"The year built min: {min(temp['year_built'])}, max: {max(df_all['year_built'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R07ioWMzTFs"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">The oldest building was built in 1600 and the latest in 2016. The majority of the buildings were built since 1900. There were some 0 and null values. Not quite sure what 0 signifies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vonHt00RzTFs"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(df_all,\n",
        "             vars=['energy_star_rating', 'floor_area', 'ELEVATION'],\n",
        "             hue='dataset',\n",
        "             height=4,\n",
        "             plot_kws = {'alpha' : 0.4, 's': 30, 'edgecolor': 'k'},\n",
        "             corner=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6jnA_LazTFs"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">`train` set buildings have higher floor areas compared to `test` set buildings and small positive correlation between `floor_area` and `energy_star_rating` can be observable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtOr_crIzTFt"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "sns.boxplot(data=df_all[['avg_temp','State_Factor']].drop_duplicates(),y='avg_temp', x='State_Factor')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIJ6gCixzTFt"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">According to the average temperature (`avg_temp`), if we list states from warmest to coldest: State 1, State 2, and State 8. The range of temperatures of State1, State 6, State 11, State 4 are higher compared to the other states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG2n83AgzTFt"
      },
      "outputs": [],
      "source": [
        "cols = [['january_min_temp', 'january_avg_temp', 'january_max_temp'],\n",
        "        ['february_min_temp', 'february_avg_temp', 'february_max_temp'],\n",
        "        ['march_min_temp', 'march_avg_temp', 'march_max_temp'],\n",
        "        ['april_min_temp','april_avg_temp', 'april_max_temp'],\n",
        "        ['may_min_temp', 'may_avg_temp','may_max_temp'],\n",
        "        ['june_min_temp', 'june_avg_temp', 'june_max_temp'],\n",
        "        ['july_min_temp', 'july_avg_temp', 'july_max_temp'],\n",
        "        ['august_min_temp','august_avg_temp', 'august_max_temp'],\n",
        "        ['september_min_temp','september_avg_temp', 'september_max_temp'],\n",
        "        ['october_min_temp','october_avg_temp', 'october_max_temp'],\n",
        "        ['november_min_temp','november_avg_temp', 'november_max_temp'],\n",
        "        ['december_min_temp','december_avg_temp', 'december_max_temp']]\n",
        "\n",
        "fig, ax = plt.subplots(2, 6, figsize=(20,6), sharey=True)\n",
        "fig.suptitle(\"Monthly temperature and number of unique values\")\n",
        "\n",
        "for e, c in enumerate(cols):\n",
        "    if e<=5:\n",
        "        sns.histplot(df_all[c].drop_duplicates(), ax=ax[0,e], legend=False)\\\n",
        "        .set(title=c[0][:c[0].find('_')]+ '_#'+str(len(df_all[c[0]].unique())))\n",
        "\n",
        "    else:\n",
        "        sns.histplot(df_all[c].drop_duplicates(), ax=ax[1,e-6], legend=False)\\\n",
        "        .set(title=c[0][:c[0].find('_')]+ '_#'+str(len(df_all[c[0]].unique())))\n",
        "\n",
        "plt.subplots_adjust(hspace=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjzwX1J5zTFt"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"background-color: pink; font-family:verdana; line-height: 1.7em;\">Each month has unique temperature values between 31 and 59 which means the values in temperature volumes are highly repeated for the data points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFYyFtTjzTFu"
      },
      "outputs": [],
      "source": [
        "cols=['cooling_degree_days','heating_degree_days','precipitation_inches', 'snowfall_inches',\n",
        "      'snowdepth_inches', 'avg_temp', 'days_below_30F', 'days_below_20F',\n",
        "      'days_below_10F', 'days_below_0F', 'days_above_80F', 'days_above_90F',\n",
        "       'days_above_100F', 'days_above_110F', 'direction_max_wind_speed',\n",
        "       'direction_peak_wind_speed', 'max_wind_speed', 'days_with_fog']\n",
        "\n",
        "fig, ax = plt.subplots(6,3, figsize=(15,18), sharey=True)\n",
        "fig.suptitle(\"Numerical variables and the number of unique values\")\n",
        "\n",
        "for e, c in enumerate(cols):\n",
        "    if e<=5:\n",
        "        sns.histplot(df_all[c].drop_duplicates(), ax=ax[e,0], legend=False)\\\n",
        "        .set(title=c+\"_#\"+str(len(df_all[c].unique())), ylabel=None, xlabel=None)\n",
        "    elif (e>6) & (e<=11):\n",
        "        sns.histplot(df_all[c].drop_duplicates(), ax=ax[e-6,1], legend=False)\\\n",
        "        .set(title=c+\"_#\"+str(len(df_all[c].unique())), ylabel=None, xlabel=None)\n",
        "    else:\n",
        "        sns.histplot(df_all[c].drop_duplicates(), ax=ax[e-12,2], legend=False)\\\n",
        "        .set(title=c+\"_#\"+str(len(df_all[c].unique())), ylabel=None, xlabel=None)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiYLqYRCzTFu"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"background-color:pink; font-family:verdana; line-height: 1.7em;\">Other weather related numerical columns also have few unique values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYsnIDAnzTFv"
      },
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">📌Target Variable Exploration (EUI)</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FM-DPhAzTFv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "cp = sns.histplot(x=df_all['site_eui'], kde=True, palette='Set2')\n",
        "ax1.set_xlabel('Target Histogram', fontsize=14)\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "sns.boxplot(y=df_all['site_eui'], palette='Set2')\n",
        "ax2.set_xlabel('Target boxplot', fontsize=14)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A_7ZGJPzTFw"
      },
      "source": [
        "Target is skewed (log-normal)\n",
        "The target has many outliers\n",
        "decision: log-transform, IQR outlier treatment\n",
        "\n",
        "perform: we do not perform any transformation (because to keep data original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vlJY22NzTFw"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
        "fig.suptitle(\"EUI by States\")\n",
        "\n",
        "sns.histplot(df_all, x='site_eui', hue='State_Factor', ax=ax[0])\\\n",
        ".set(title='EUI by State', ylabel=None)\n",
        "\n",
        "sns.histplot(df_all[df_all['State_Factor']!='State_6'], x='site_eui', hue='State_Factor', ax=ax[1])\\\n",
        ".set(title='EUI by State (State 6 removed)', ylabel=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubTRINVfzTFw"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">State 2 and 4 have slightly higher EUI and State 11 and 8 have lower EUI level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bjCtaRozTFx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
        "fig.suptitle(\"EUI by State and building class\")\n",
        "\n",
        "sns.violinplot(data=df_all, y='site_eui', x='State_Factor', ax=ax[0])\n",
        "sns.violinplot(data=df_all, y='site_eui', x='building_class', ax=ax[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FMhFgdxzTFx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(20,5))\n",
        "fig.suptitle(\"EUI by facility type\")\n",
        "ax=sns.boxplot(data=df_all, y='site_eui', x='facility_type')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB6tw24FzTFx"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">Labs and Data Centers have higher EUI compared to the other types of buildings. Grocery stores, Health Care Inpatient, Health Care Uncategorized, Health Care Outpatient, and Food service, restaurants have higher range of EUI. It could be the essential services must operate for longer hours, therefore, have higher EUI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bELc2Zq9zTFx"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,4, figsize=(20,5))\n",
        "\n",
        "for e, col in enumerate(['floor_area', 'year_built', 'energy_star_rating', 'ELEVATION']):\n",
        "    if col == 'year_built':\n",
        "        sns.scatterplot(data=df_all[(df_all['year_built']!=0) & (df_all['year_built'].notna())],\n",
        "                        x=col, y='site_eui', ax=ax[e]).set(title='EUI by'+ col, ylabel=None)\n",
        "    else:\n",
        "        sns.scatterplot(data=df_all, x=col, y='site_eui', ax=ax[e]).set(title='EUI by'+ col, ylabel=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TgdPCy5zTFy"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"background-color:pink; font-family:verdana; line-height: 1.7em;\">`floor_area` could have positive relationship with `EUI`. The younger buildings tend to have higher `EUI` which could be because building height and size have increased over the years. It looks like the Higher the `energy_star_rating` is, the lower the `EUI` becomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TehOxsG9zTFy"
      },
      "outputs": [],
      "source": [
        "cols = [['cooling_degree_days','heating_degree_days', 'precipitation_inches', 'snowfall_inches','snowdepth_inches'],\n",
        "        ['avg_temp', 'direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed','days_with_fog']]\n",
        "\n",
        "fig, ax = plt.subplots(2,5, figsize=(20,8))\n",
        "fig.suptitle('EUI by numerical columns')\n",
        "\n",
        "for e1, l in enumerate(cols):\n",
        "    for e2, col in enumerate(l):\n",
        "        sns.scatterplot(data=df_all,\n",
        "                        x=col, y='site_eui', ax=ax[e1, e2]).set(ylabel=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFC2aqLzzTFy"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">The most data points are in lower number of `cooling_degree_days` and higher number of `heating_degree_days`. The majority of the datapoints are also in the lower levels of `snowfall_inches` and `snowdepth_inches`.`direction_max_wind_speed`, `direction_peak_wind_speed`, `max_wind_speed`, and `days_with_fog` columns have the `NA` values of over 50%. No relationship between `EUI` and the weather related numerica columns can be observed from the plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW-Id3yAzTFz"
      },
      "source": [
        "<div style=\"border-radius:10px; border:#DEB887 solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n",
        "<h3 align=\"left\"><font color='#DEB887'>💡 Observations:</font></h3>\n",
        "\n",
        "- Categorical variables such as `State_Factor`, `building_class` and `facility_type` might have some correlation with `EUI`.\n",
        "- `State_6` is not present in `test` set. State 6 removed training data should be tested.\n",
        "- `floor_area`, `energy_star_rating` should be included in the modelling to be tested.\n",
        "- From the plots, it's difficult to observe direct(linear) relationship between `EUI` and weather related variables. However this doesn't deny non-linear relationships among the variables.\n",
        "- Variables with more than 50% `NA` values should not be imputed(in my opinion) and better to be not included in the training set.\n",
        "- Weather variables have few unique values repeated throughout the datapoints. Not sure how this duplicated values might affect the modeling and prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOmmUmPYzTFz"
      },
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">⚙️ Preprocessing</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EghvU76zTFz"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">we will suppose that two if two buildings have the same values for these features; they are the same building, in other words groupby_cols = (building_id)\n",
        "Removing duplicates by clubbing similar building data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aezVPHyFzTFz"
      },
      "outputs": [],
      "source": [
        "groupby_cols = ['State_Factor','building_class','facility_type','floor_area','year_built']\n",
        "df_all = df_all.sort_values(by=groupby_cols+['Year_Factor']).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGhm1jAtzTF0"
      },
      "outputs": [],
      "source": [
        "df_all.loc[:,df_all.dtypes=='object'].columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKQt6fsTzTF0"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">Null imputation for categorical values: <b>KNN Imputing</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4G39zdyzTF0"
      },
      "outputs": [],
      "source": [
        "df_all_one_hot = df_all.copy()\n",
        "\n",
        "cats = ['State_Factor', 'facility_type', 'building_class']\n",
        "for col in cats:\n",
        "    dummies = pd.get_dummies(df_all_one_hot[col], dummy_na=False)\n",
        "    for ohe_col in dummies:\n",
        "        df_all_one_hot[f'ohe_{col}_{ohe_col}'] = dummies[ohe_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWy7mR1szTF0"
      },
      "outputs": [],
      "source": [
        "df_all_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le2JZly8zTF1"
      },
      "outputs": [],
      "source": [
        "knn_imputing = False\n",
        "target='site_eui'\n",
        "\n",
        "if knn_imputing:\n",
        "    imputer = KNNImputer(n_neighbors=7)\n",
        "    tmp = df_all[['State_Factor', 'building_class', 'facility_type', 'dataset', target]]\n",
        "    df = df_all.drop(tmp.columns, axis=1)\n",
        "    df1 = pd.DataFrame(imputer.fit_transform(df),columns = df.columns)\n",
        "    tmp.to_csv('imputer_tmp.csv', index=False)\n",
        "    df1.to_csv('imputer_df1.csv', index=False)\n",
        "    joblib.dump(imputer, 'knn_imputer.pkl')\n",
        "\n",
        "else:\n",
        "    df1 = pd.read_csv(r'/content/imputer_df1.csv')\n",
        "    tmp = df_all[['State_Factor', 'building_class', 'facility_type', 'dataset', target]]\n",
        "    df_all = df_all.drop(tmp.columns, axis=1)\n",
        "    for col in tmp.columns:\n",
        "        df_all[col]=tmp[col]\n",
        "    for col in df1.columns:\n",
        "        df_all[col] = df1[col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHrvCB0jzTF1"
      },
      "outputs": [],
      "source": [
        "df_all_te = df_all.copy()\n",
        "\n",
        "cats = ['State_Factor', 'building_class', 'facility_type']\n",
        "for col in cats:\n",
        "    encoder = TargetEncoder()\n",
        "    df_all_te[f'te_{col}'] = encoder.fit_transform(df_all_te[col], df_all_te[target])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZoBz5xuzTF1"
      },
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">Feature Engineering</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlY0u2ePzTF1"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n",
        "    <font size='3.5pt'><b>Weather based features</b></font>\n",
        "\n",
        "we will extract new weather statistics from the building location weather features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PibtqaFmzTF2"
      },
      "outputs": [],
      "source": [
        "# extract new weather statistics from the building location weather features\n",
        "temp = [col for col in df_all_te.columns if 'temp' in col]\n",
        "\n",
        "df_all_te['min_temp'] = df_all_te[temp].min(axis=1)\n",
        "df_all_te['max_temp'] = df_all_te[temp].max(axis=1)\n",
        "df_all_te['avg_temp'] = df_all_te[temp].mean(axis=1)\n",
        "df_all_te['std_temp'] = df_all_te[temp].std(axis=1)\n",
        "df_all_te['skew_temp'] = df_all_te[temp].skew(axis=1)\n",
        "\n",
        "\n",
        "# by seasons\n",
        "temp = pd.Series([col for col in df_all_te.columns if 'temp' in col])\n",
        "\n",
        "winter_temp = temp[temp.apply(lambda x: ('january' in x or 'february' in x or 'december' in x))].values\n",
        "spring_temp = temp[temp.apply(lambda x: ('march' in x or 'april' in x or 'may' in x))].values\n",
        "summer_temp = temp[temp.apply(lambda x: ('june' in x or 'july' in x or 'august' in x))].values\n",
        "autumn_temp = temp[temp.apply(lambda x: ('september' in x or 'october' in x or 'november' in x))].values\n",
        "\n",
        "\n",
        "### winter\n",
        "df_all_te['min_winter_temp'] = df_all_te[winter_temp].min(axis=1)\n",
        "df_all_te['max_winter_temp'] = df_all_te[winter_temp].max(axis=1)\n",
        "df_all_te['avg_winter_temp'] = df_all_te[winter_temp].mean(axis=1)\n",
        "df_all_te['std_winter_temp'] = df_all_te[winter_temp].std(axis=1)\n",
        "df_all_te['skew_winter_temp'] = df_all_te[winter_temp].skew(axis=1)\n",
        "### spring\n",
        "df_all_te['min_spring_temp'] = df_all_te[spring_temp].min(axis=1)\n",
        "df_all_te['max_spring_temp'] = df_all_te[spring_temp].max(axis=1)\n",
        "df_all_te['avg_spring_temp'] = df_all_te[spring_temp].mean(axis=1)\n",
        "df_all_te['std_spring_temp'] = df_all_te[spring_temp].std(axis=1)\n",
        "df_all_te['skew_spring_temp'] = df_all_te[spring_temp].skew(axis=1)\n",
        "### summer\n",
        "df_all_te['min_summer_temp'] = df_all_te[summer_temp].min(axis=1)\n",
        "df_all_te['max_summer_temp'] = df_all_te[summer_temp].max(axis=1)\n",
        "df_all_te['avg_summer_temp'] = df_all_te[summer_temp].mean(axis=1)\n",
        "df_all_te['std_summer_temp'] = df_all_te[summer_temp].std(axis=1)\n",
        "df_all_te['skew_summer_temp'] = df_all_te[summer_temp].skew(axis=1)\n",
        "### autumn\n",
        "df_all_te['min_autumn_temp'] = df_all_te[summer_temp].min(axis=1)\n",
        "df_all_te['max_autumn_temp'] = df_all_te[summer_temp].max(axis=1)\n",
        "df_all_te['avg_autumn_temp'] = df_all_te[summer_temp].mean(axis=1)\n",
        "df_all_te['std_autumn_temp'] = df_all_te[summer_temp].std(axis=1)\n",
        "df_all_te['skew_autumn_temp'] = df_all_te[summer_temp].skew(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBuEiHgLzTF2"
      },
      "outputs": [],
      "source": [
        "df_all_te['month_cooling_degree_days'] = df_all_te['cooling_degree_days']/12\n",
        "df_all_te['month_heating_degree_days'] = df_all_te['heating_degree_days']/12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6WD12-vzTF2"
      },
      "outputs": [],
      "source": [
        "df_all_te[temp]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GUe3j0tzTF2"
      },
      "outputs": [],
      "source": [
        "df_all_te['cooling_degree_days']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZYBvDAZTzTF3"
      },
      "outputs": [],
      "source": [
        "df_all_te[['min_winter_temp', 'max_winter_temp', 'avg_winter_temp', 'std_winter_temp', 'skew_winter_temp',\n",
        "           'min_spring_temp', 'max_spring_temp', 'avg_spring_temp', 'std_spring_temp', 'skew_spring_temp',\n",
        "           'min_summer_temp', 'max_summer_temp', 'avg_summer_temp', 'std_summer_temp', 'skew_summer_temp',\n",
        "           'min_autumn_temp', 'max_autumn_temp', 'avg_autumn_temp', 'std_autumn_temp', 'skew_autumn_temp']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPMD7eetzTF3"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n",
        "    <font size='3.5pt'><b>Buildig based feature:</b></font>\n",
        "\n",
        "we will extract building statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOyfC_0szTF3"
      },
      "outputs": [],
      "source": [
        "# total area\n",
        "df_all_te['building_area'] = df_all_te['floor_area'] * df_all_te['ELEVATION']\n",
        "# rating energy by floor\n",
        "df_all_te['floor_energy_star_rating'] = df_all_te['energy_star_rating']/df_all_te['ELEVATION']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JnFikiQzTF4"
      },
      "outputs": [],
      "source": [
        "df_all_te[[\"floor_area\", \"ELEVATION\", \"energy_star_rating\", \"floor_energy_star_rating\", \"building_area\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4dppbVZzTF4"
      },
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">Checking target variable transformation</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mvkEDrPzTF4"
      },
      "outputs": [],
      "source": [
        "target = 'site_eui'\n",
        "plt.figure(figsize=(10,7))\n",
        "# plot the original variable vs sale price\n",
        "plt.subplot(2,1,1)\n",
        "train[target].hist(bins=50)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Original ' + target)\n",
        "\n",
        "# plot transformed variable vs sale price\n",
        "plt.subplot(2,1,2)\n",
        "np.log(train[target]).hist(bins=50)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Transformed ' + target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGvRbjghzTF4"
      },
      "outputs": [],
      "source": [
        "nums = train.loc[:, train.dtypes != 'object'].columns\n",
        "df_all[nums].hist(bins=50, figsize=(20,20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELGfdnC-zTF5"
      },
      "outputs": [],
      "source": [
        "df_all_te[nums].skew().sort_values(key=abs, ascending=False)[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cH0jH7sHzTF5"
      },
      "source": [
        "#### Binarize very skewed variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVVPmM-KzTF5"
      },
      "outputs": [],
      "source": [
        "skewed = ['days_above_110F', 'days_above_100F']\n",
        "\n",
        "for var in skewed:\n",
        "\n",
        "    # map the variable values into 0 and 1\n",
        "    df_all_te[var] = np.where(df_all_te[var]==0, 0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay8l9Ji8zTF5"
      },
      "source": [
        "#### Saving feature dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qSfZ6xlzTF6"
      },
      "outputs": [],
      "source": [
        "saved = True\n",
        "if saved:\n",
        "    !pip install pickle5 --quiet\n",
        "    import pickle5 as pickle\n",
        "    data_path = 'feature_transformed_set_TEonly.pkl'\n",
        "    with open(data_path, \"rb\") as fh:\n",
        "        df = pickle.load(fh)\n",
        "else:\n",
        "    df_all.to_pickle('feature_transformed_set_TEonly.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">🧰 Baseline Modelling</span>"
      ],
      "metadata": {
        "id": "6swk10mt6FtN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-wUSucXzTF6"
      },
      "outputs": [],
      "source": [
        "# cats = ['State_Factor', 'facility_type', 'building_class', 'days_above_100F', 'days_above_110F']\n",
        "\n",
        "# typecasting numerical features\n",
        "for col in df_all_te.drop(columns=['State_Factor', 'facility_type', 'building_class','dataset', 'id', 'site_eui', 'days_above_100F', 'days_above_110F']).columns:\n",
        "    df_all_te[col] = df_all_te[col].astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_all_te[df_all_te['dataset']=='train']\n",
        "test = df_all_te[df_all_te['dataset']=='test']\n",
        "\n",
        "train = train.reset_index(drop=True)\n",
        "test = test.reset_index(drop=True)\n",
        "\n",
        "test_ids = test['id']\n",
        "train_ids = train['id']\n",
        "\n",
        "target = train['site_eui'] #y_train\n",
        "\n",
        "train = train.drop(['id', 'dataset', 'site_eui'], axis=1) #X_train\n",
        "test = test.drop(['id', 'dataset', 'site_eui'], axis=1) #X_test\n",
        "X_train = X_train.drop(columns=['State_Factor', 'facility_type', 'building_class'], axis=1)\n",
        "X_test = X_test.drop(columns=['State_Factor', 'facility_type', 'building_class'], axis=1)"
      ],
      "metadata": {
        "id": "ZN6B_gazOwJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = pd.read_csv('y_test.csv')\n",
        "\n",
        "X_train = train\n",
        "X_test = test\n",
        "y_train = target\n",
        "y_test = y_test['site_eui']\n",
        "print('Train: ', X_train.shape)\n",
        "print('Test:', X_test.shape)\n",
        "print('Samples: ', y_train.shape)\n",
        "print('Targets: ', y_test.shape)"
      ],
      "metadata": {
        "id": "cmbycDT3O6EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">RandomForest</span>"
      ],
      "metadata": {
        "id": "V7t9GdMCQgSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor(random_state=1, criterion='squared_error', max_depth = 15, min_samples_split= 2)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)"
      ],
      "metadata": {
        "id": "s1Ma-mrzQV98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Training data scores\\n\",\"--\"*10)\n",
        "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
        "print(\" MAE:\", mean_absolute_error(y_test,y_pred))\n",
        "print(\" MSE:\", mean_squared_error(y_test,y_pred))\n",
        "print(\" R2:\", r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "B77lzf4GQZ_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">Lasso Regression</span>"
      ],
      "metadata": {
        "id": "vWyiav-PQdxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lasso = Lasso()\n",
        "lasso.fit(X_train, y_train)\n",
        "y_pred = lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "hPpvvOzsQbdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" Training data scores\\n\",\"--\"*10)\n",
        "print(\" RMSE:\", np.sqrt(mean_squared_error(y_test,y_pred)))\n",
        "print(\" MAE:\", mean_absolute_error(y_test,y_pred))\n",
        "print(\" MSE:\", mean_squared_error(y_test,y_pred))\n",
        "print(\" R2:\", r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "UmB9Jyp_QiUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">Model Evaluation</span>"
      ],
      "metadata": {
        "id": "1ItQTvLFQlzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error_rec = {\n",
        "    \"randomforest\": {\n",
        "        \"mae\": 50.1839889322522,\n",
        "        \"rmse\": 79.12972350643851,\n",
        "    },\n",
        "    \"lasso\": {\n",
        "         \"mae\": 43.26164501092259,\n",
        "        \"rmse\": 66.08328222674918,\n",
        "    },\n",
        "}\n",
        "pd.DataFrame(error_rec).plot(kind=\"bar\",\n",
        "             color=[\n",
        "                 sns.color_palette(\"pastel\")[0],\n",
        "                 sns.color_palette(\"pastel\")[1],\n",
        "                 sns.color_palette(\"pastel\")[2],\n",
        "                 sns.color_palette(\"pastel\")[3]]);"
      ],
      "metadata": {
        "id": "-cInjZ69Qjip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">Hyperparameter Tuning</span>"
      ],
      "metadata": {
        "id": "BLGlISiXQps0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepaere the out of folds predictions\n",
        "train_oof = np.zeros((train.shape[0],))\n",
        "test_preds = np.zeros(test.shape[0])\n",
        "\n",
        "NUM_FOLDS = 5\n",
        "kf = KFold(n_splits = NUM_FOLDS, shuffle=True, random_state=0)\n",
        "\n",
        "\n",
        "for fold, (train_idx, test_idx) in tqdm.tqdm(enumerate(kf.split(train, target))):\n",
        "    X_train, X_test = train.iloc[train_idx][test.columns], train.iloc[test_idx][test.columns]\n",
        "    y_train, y_test = target[train_idx], target[test_idx]\n",
        "\n",
        "    catb = CatBoostRegressor(iterations=500,\n",
        "                         learning_rate=0.02,\n",
        "                         depth=12,\n",
        "                         eval_metric='RMSE',\n",
        "#                         early_stopping_rounds=42,\n",
        "                         random_seed = 23,\n",
        "                         bagging_temperature = 0.2,\n",
        "                         od_type='Iter',\n",
        "                         metric_period = 75,\n",
        "                         od_wait=100)\n",
        "    # train model\n",
        "    catb.fit(X_train, y_train,\n",
        "                 eval_set=(X_test,y_test),\n",
        "                 cat_features=cats_discrete_idx,\n",
        "                 use_best_model=True,\n",
        "                 verbose=True)\n",
        "\n",
        "    oof = catb.predict(X_test)\n",
        "    train_oof[test_idx] = oof\n",
        "    test_preds += catb.predict(test)/NUM_FOLDS\n",
        "    print(f\"out-of-folds prdiction ==== fold_{fold} RMSE\",np.sqrt(mean_squared_error(oof, y_test, squared=False)))\n"
      ],
      "metadata": {
        "id": "ujhQ0Vh3Qng_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">Using Optuna with Random Forest</span>"
      ],
      "metadata": {
        "id": "cJ0uqXcyo2yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = train.drop(columns=['State_Factor', 'facility_type', 'building_class'], axis = 1)\n",
        "X_train = X_train.drop(columns=['State_Factor', 'facility_type', 'building_class'], axis = 1)\n",
        "X_test = X_test.drop(columns=['State_Factor', 'facility_type', 'building_class'], axis = 1)"
      ],
      "metadata": {
        "id": "OLbmkyFstQzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "mNS65BCxtpaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cross validating training data\n",
        "kfolds = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Objective function\n",
        "def random_forest_objective(trial, data=X_train, target=y_train):\n",
        "    # Dictionary to store best parameters\n",
        "    param = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
        "        \"max_features\": trial.suggest_float(\"max_features\", 0.01, 0.95)\n",
        "    }\n",
        "\n",
        "    model = RandomForestRegressor(**param)\n",
        "\n",
        "    # Setting random seed and kfolds for cross-validation\n",
        "    kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    scores = cross_val_score(model, data, target, cv=kfolds, scoring=\"neg_root_mean_squared_error\")\n",
        "    return scores.mean()\n",
        "\n"
      ],
      "metadata": {
        "id": "UaTWNN12oxLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tuner(objective, n=5, direction='minimize'):\n",
        "    # Create Study object\n",
        "    study = optuna.create_study(direction=\"minimize\")\n",
        "\n",
        "    # Optimize the study\n",
        "    study.optimize(objective, n_trials=n)\n",
        "\n",
        "    # Print the result\n",
        "    best_params = study.best_params\n",
        "    best_score = study.best_value\n",
        "    print(f\"Best score: {best_score}\")\n",
        "    print(f\"Optimized parameters: {best_params}\\n\")\n",
        "    print(\"<<<<<<<<<< Tuning complete... >>>>>>>>>>\")\n",
        "\n",
        "    # Return best parameters for the model\n",
        "    return best_params, best_score"
      ],
      "metadata": {
        "id": "7YjaiegFo4sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "rf_param, rf_score = tuner(random_forest_objective,1)\n",
        "rf_tuned_model = RandomForestRegressor(**rf_param)"
      ],
      "metadata": {
        "id": "ChvhjVG0o8Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_tuned_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7sb8zyej0ohD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">Final Evaluation</span>"
      ],
      "metadata": {
        "id": "GvgPnvA23MTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_tuned = catb.predict(X_test)\n",
        "\n",
        "plt.figure(figsize = (7,5))\n",
        "sns.displot(y_test - y_hat_tuned)\n",
        "plt.title(\"Error Rate Distribution\");\n",
        "plt.ylabel(\"error\")\n",
        "plt.xlabel(\"iteration\")"
      ],
      "metadata": {
        "id": "W1ylXBhy3Lt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat_tuned = rf_tuned_model.predict(X_test)\n",
        "\n",
        "plt.figure(figsize = (7,5))\n",
        "sns.displot(y_test - y_hat_tuned)\n",
        "plt.title(\"Error Rate Distribution\");\n",
        "plt.ylabel(\"error\")\n",
        "plt.xlabel(\"iteration\")"
      ],
      "metadata": {
        "id": "ByyPmGumyi9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <span style=\"font-family:cursive;text-align:center\">Saving Models</span>"
      ],
      "metadata": {
        "id": "JnPUu2wn2Ylh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('rf_tuned_final.joblib', 'wb') as f:\n",
        "    joblib.dump(rf_tuned_model, f, compress=1)\n",
        "\n",
        "with open('catb_best_final.joblib', 'wb') as f:\n",
        "    joblib.dump(catb,f,compress=3)"
      ],
      "metadata": {
        "id": "HnpMvBWeykYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load models here\n",
        "load = False\n",
        "if load:\n",
        "    with open('rf_tuned_final.joblib', 'rb') as f:\n",
        "        rf = joblib.load(f)\n",
        "\n",
        "    with open('catb_best_final.joblib', 'rb') as f:\n",
        "        catb = joblib.load(f)"
      ],
      "metadata": {
        "id": "DXzqCCKKzAKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span style=\"font-family:cursive;text-align:center\">Explainable AI</span>"
      ],
      "metadata": {
        "id": "ySqjYQqB19lX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wwS7Al7Y2l8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}